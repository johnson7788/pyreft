# Jailbreak with ReFT

> [!WARNING]  
> This study includes content that might enable individuals to create damaging material using certain public large language models (LLMs). Although there are inherent risks, we think it's crucial to fully reveal this research. We insist ReFT should only be used to study model's safety features, and must not use ReFT to distribute models that can generate harmful content at scale. We believe ReFT would eventually enhance model safety.